# triton-windows-amdgpu
Triton pre-build wheel for amd gpu in windows (zluda)

source: https://github.com/Repeerc/triton-amd-windows fork from `triton-lang/triton`

test work with Python 3.10 + Torch 2.2.1+cu118 (zluda, win11)

`.whl` file see Release

# Test

GPU: AMD RX 7900XTX (gfx1100)

`zluda -- python .\06-fused-attention.py`

```
fused-attention-batch4-head32-d64-fwd-causal=True:
     N_CTX  Triton [FP16]  Triton [FP8]
0   1024.0      18.605487     16.161252
1   2048.0      23.365859     19.112081
2   4096.0      24.773427     20.967591
3   8192.0      26.135536     22.287300
4  16384.0      26.630619     22.616694
fused-attention-batch4-head32-d64-fwd-causal=False:
     N_CTX  Triton [FP16]  Triton [FP8]
0   1024.0      25.815977     21.370182
1   2048.0      24.925183     20.253732
2   4096.0      26.949606     22.225510
3   8192.0      27.020272     22.018438
4  16384.0      28.638289     21.999093
fused-attention-batch4-head32-d64-bwd-causal=True:
     N_CTX  Triton [FP16]  Triton [FP8]
0   1024.0      17.197497     18.426106
1   2048.0      23.819577     23.042882
2   4096.0      25.709931     25.977297
3   8192.0      28.091048     28.174547
4  16384.0      28.933801     28.706476
```

`zluda -- python .\03-matrix-multiplication.py`

```
triton_output_with_fp16_inputs=tensor([[-30.4844,   7.5391, -11.2500,  ..., -16.4531,  -5.3398,   3.8965],
        [ 52.5938, -11.8125, -29.0312,  ..., -32.6250, -23.9531, -27.7812],
        [ 22.6562, -21.3438,   1.1143,  ...,  -2.9609, -78.3750,  34.2812],
        ...,
        [-13.5234,  27.5469,  -8.5234,  ...,  -6.9727,   9.0781,  18.9219],
        [  1.8652,   9.3516,   6.7500,  ..., -16.6250, -21.3125, -19.4531],
        [-17.5938, -28.5938,   0.5933,  ...,  22.0000,  -3.4648,  43.1250]],
       device='cuda:0', dtype=torch.float16)
torch_output_with_fp16_inputs=tensor([[-30.4844,   7.5391, -11.2500,  ..., -16.4531,  -5.3398,   3.8965],
        [ 52.5938, -11.8125, -29.0312,  ..., -32.6250, -23.9531, -27.7812],
        [ 22.6562, -21.3438,   1.1143,  ...,  -2.9609, -78.3750,  34.2812],
        ...,
        [-13.5234,  27.5469,  -8.5234,  ...,  -6.9727,   9.0781,  18.9219],
        [  1.8652,   9.3516,   6.7500,  ..., -16.6250, -21.3125, -19.4531],
        [-17.5938, -28.5938,   0.5933,  ...,  22.0000,  -3.4648,  43.1250]],
       device='cuda:0', dtype=torch.float16)
âœ… Triton and Torch match
matmul-performance-fp16:
         M       N       K     rocBLAS      Triton
0    256.0   256.0   256.0   67.108861   67.108861
1    384.0   384.0   384.0  283.115527  102.951099
2    512.0   512.0   512.0  536.870887   30.504029
3    640.0   640.0   640.0  873.813292   36.408890
4    768.0   768.0   768.0   94.371836   34.578995
5    896.0   896.0   896.0   53.680833   47.168730
6   1024.0  1024.0  1024.0   43.383508   42.608802
7   1152.0  1152.0  1152.0   66.615419   58.688055
8   1280.0  1280.0  1280.0   62.137835   52.958384
9   1408.0  1408.0  1408.0   71.663911   57.023682
10  1536.0  1536.0  1536.0   64.367295   58.829199
11  1664.0  1664.0  1664.0   72.960296   65.076879
12  1792.0  1792.0  1792.0   71.485530   65.430190
13  1920.0  1920.0  1920.0   82.685604   72.556517
14  2048.0  2048.0  2048.0   80.884505   73.685905
15  2176.0  2176.0  2176.0   84.315123   74.960409
16  2304.0  2304.0  2304.0  103.913255   74.850617
17  2432.0  2432.0  2432.0  106.511408   75.055389
18  2560.0  2560.0  2560.0  102.534550   74.915008
19  2688.0  2688.0  2688.0  104.389814   77.469984
20  2816.0  2816.0  2816.0  104.543422   76.369611
21  2944.0  2944.0  2944.0  105.069169   77.633067
22  3072.0  3072.0  3072.0   97.893059   72.627369
23  3200.0  3200.0  3200.0  102.488074   78.514437
24  3328.0  3328.0  3328.0  102.959616   78.278830
25  3456.0  3456.0  3456.0  104.330198   78.891962
26  3584.0  3584.0  3584.0  105.335040   78.775977
27  3712.0  3712.0  3712.0  107.008607   79.129669
28  3840.0  3840.0  3840.0  107.464613   80.191340
29  3968.0  3968.0  3968.0  104.505928   79.569845
30  4096.0  4096.0  4096.0  107.761447   79.917988

```
